# TTS Evaluation Pipeline Configuration

# Device settings
device: "cuda"  # "cuda" or "cpu"
gpu_index: 0

# Output directory
output_dir: "./outputs"

# Models to evaluate
models:
  - name: "glm_tts"
    type: "glm_tts"
    enabled: true
    model_path: "./GLM-TTS/ckpt"
    use_phoneme: false
    use_cache: true
    # Voice cloning with Yang's voice (male)
    prompt_wav: "./processed_audio/normalized_Yang.wav"
    prompt_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."

  - name: "glm_tts_rl"
    type: "glm_tts_rl"
    enabled: true
    model_path: "./GLM-TTS/ckpt"
    rl_ckpt_path: null  # Uses default if null
    # Voice cloning with Yang's voice (male)
    prompt_wav: "./processed_audio/normalized_Yang.wav"
    prompt_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."

  - name: "qwen_tts"
    type: "qwen_tts"
    enabled: true
    model_id: "Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice"
    speaker: "Vivian"
    language: "English"
    use_flash_attn: true

  # Qwen3-TTS Voice Cloning (requires Base model)
  - name: "qwen_tts_vc"
    type: "qwen_tts_vc"
    enabled: true  # Voice cloning with Yang's voice
    model_id: "Qwen/Qwen3-TTS-12Hz-0.6B-Base"
    language: "auto"
    ref_audio: "./processed_audio/normalized_Yang.wav"
    ref_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."
    x_vector_only: false  # If true, only use speaker embedding (no ICL)

  # Qwen3-TTS with vLLM-Omni acceleration + Voice Cloning (Linux/WSL only)
  - name: "qwen_tts_vllm"
    type: "qwen_tts_vllm_vc"
    enabled: true
    model_id: "Qwen/Qwen3-TTS-12Hz-0.6B-Base"  # Base model for voice cloning
    language: "English"
    # Voice cloning with Yang's voice (male)
    ref_audio: "./processed_audio/normalized_Yang.wav"
    ref_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."
    mode: "icl"  # "icl" for in-context learning, "xvec_only" for x-vector only
    # vLLM-specific parameters
    temperature: 0.9
    top_p: 1.0
    top_k: 50
    max_tokens: 2048
    repetition_penalty: 1.05

  # Qwen3-TTS vLLM single-batch mode (one request at a time)
  - name: "qwen_tts_vllm_single"
    type: "qwen_tts_vllm_vc"
    enabled: false  # Enable for single-batch vs batch comparison
    model_id: "Qwen/Qwen3-TTS-12Hz-0.6B-Base"
    language: "English"
    ref_audio: "./processed_audio/normalized_Yang.wav"
    ref_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."
    mode: "icl"
    single_batch: true  # Process one request at a time
    temperature: 0.9
    top_p: 1.0
    top_k: 50
    max_tokens: 2048
    repetition_penalty: 1.05

  # CosyVoice with zero-shot voice cloning
  - name: "cosyvoice"
    type: "cosyvoice"
    enabled: true
    model_path: "./CosyVoice/pretrained_models/CosyVoice2-0.5B"
    repo_path: "./CosyVoice"
    mode: "zero_shot"  # zero_shot, cross_lingual, instruct
    # Voice cloning settings (zero_shot mode)
    prompt_wav: "./processed_audio/normalized_Yang.wav"
    prompt_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."

  - name: "cosyvoice_rl"
    type: "cosyvoice_rl"
    enabled: true
    model_path: "./CosyVoice/pretrained_models/CosyVoice2-0.5B"
    repo_path: "./CosyVoice"
    mode: "zero_shot"
    # Voice cloning with Yang's voice (male)
    prompt_wav: "./processed_audio/normalized_Yang.wav"
    prompt_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."


  # CosyVoice with vLLM acceleration (Linux/WSL only)
  # NOTE: First run has ~30s warmup, then RTF ~0.2-0.4 (stable state)
  # Requires onnxruntime-gpu for best performance
  - name: "cosyvoice_vllm"
    type: "cosyvoice_vllm"
    enabled: true

    model_path: "./CosyVoice/pretrained_models/CosyVoice2-0.5B"
    repo_path: "./CosyVoice"
    mode: "zero_shot"
    fp16: false
    load_trt: false
    # Voice cloning with Yang's voice (male)
    prompt_wav: "./processed_audio/normalized_Yang.wav"
    prompt_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."

  # CosyVoice vLLM single-batch mode (one request at a time)
  - name: "cosyvoice_vllm_single"
    type: "cosyvoice_vllm"
    enabled: false  # Enable for single-batch vs batch comparison
    model_path: "./CosyVoice/pretrained_models/CosyVoice2-0.5B"
    repo_path: "./CosyVoice"
    mode: "zero_shot"
    fp16: false
    load_trt: false
    single_batch: true  # Process one request at a time (no streaming)
    prompt_wav: "./processed_audio/normalized_Yang.wav"
    prompt_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."

  # ========================
  # CosyVoice vLLM API (connect to running CosyVoice server)
  # Start: conda run -n tts-cosyvoice-vllm python cosyvoice_server_vllm.py --port 50000
  # ========================

  - name: "cosyvoice_vllm_api"
    type: "cosyvoice_vllm_api"
    enabled: true
    api_base: "http://localhost:50000"
    mode: "zero_shot"
    prompt_wav: "./processed_audio/normalized_Yang.wav"
    prompt_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."
    api_timeout: 300.0

  # ========================
  # API-based models (connect to running vLLM-Omni server)
  # Requires: vllm-omni serve running on the specified host:port
  # ========================

  # Qwen3-TTS via API (CustomVoice - predefined speaker)
  - name: "qwen_tts_api"
    type: "qwen_tts_api"
    enabled: false
    api_base: "http://localhost:8000"
    api_key: "EMPTY"
    model_id: "Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice"
    speaker: "Vivian"
    language: "English"
    max_new_tokens: 2048
    api_timeout: 300.0

  # Qwen3-TTS via API (Voice Cloning with Yang's voice - Base model)
  - name: "qwen_tts_api_vc"
    type: "qwen_tts_api_vc"
    enabled: false
    api_base: "http://localhost:8000"
    api_key: "EMPTY"
    model_id: "Qwen/Qwen3-TTS-12Hz-0.6B-Base"
    language: "English"
    ref_audio: "./processed_audio/normalized_Yang.wav"
    ref_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."
    mode: "icl"  # "icl" or "xvec_only"
    max_new_tokens: 2048
    api_timeout: 300.0

  # Qwen3-TTS API VC with concurrency=2 (batch_size=2)
  - name: "qwen_tts_api_vc_batch2"
    type: "qwen_tts_api_vc"
    enabled: false
    api_base: "http://localhost:8000"
    api_key: "EMPTY"
    model_id: "Qwen/Qwen3-TTS-12Hz-0.6B-Base"
    language: "English"
    ref_audio: "./processed_audio/normalized_Yang.wav"
    ref_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."
    mode: "icl"
    concurrency: 2
    max_new_tokens: 2048
    api_timeout: 300.0

  # Qwen3-TTS API VC with concurrency=5 (batch_size=5)
  - name: "qwen_tts_api_vc_batch5"
    type: "qwen_tts_api_vc"
    enabled: false
    api_base: "http://localhost:8000"
    api_key: "EMPTY"
    model_id: "Qwen/Qwen3-TTS-12Hz-0.6B-Base"
    language: "English"
    ref_audio: "./processed_audio/normalized_Yang.wav"
    ref_text: "Hello everyone, welcome back to CS294-137. Today, we will continue our discussion in computer vision."
    mode: "icl"
    concurrency: 5
    max_new_tokens: 2048
    api_timeout: 300.0

# Test scripts
# Use --scripts (-s) flag to load from external file
# Default: scripts/tts_evaluation_dataset.json (40 samples)
#
# Usage:
#   python main.py -s scripts/tts_evaluation_dataset.json
#
# Dataset categories: code, notes, math, tutor
# See scripts/tts_evaluation_dataset.json for full details
scripts_file: "scripts/tts_evaluation_dataset.json"

# Fallback inline scripts (used if no --scripts flag provided)
scripts:
  - id: "test_001"
    text: "Hello, this is a test of the text to speech system."
  - id: "test_002"
    text: "The quick brown fox jumps over the lazy dog."

# Resource monitoring settings
resource_monitor:
  interval: 0.5  # seconds between samples
  gpu_index: 0
